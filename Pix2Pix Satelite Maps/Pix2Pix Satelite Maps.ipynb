{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c1daaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpolo\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(f\"tensorflow version: {tf.__version__}\")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, LeakyReLU, BatchNormalization, Concatenate, Activation, Dropout,Conv2DTranspose\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from IPython import display\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from pathlib import Path\n",
    "import imghdr\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from tensorflow.keras.utils import plot_model\n",
    "###Dataset http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d46d7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Define the Discriminator\n",
    "###The 70 × 70 discriminator architecture is:\n",
    "###C64-C128-C256-C512\n",
    "### PatchGAN is implemented by this equation:\n",
    "### (out_dim -1)*s= input_dim -k\n",
    "\n",
    "def build_discriminator():\n",
    "    source_image= Input(shape=(256, 256, 3))\n",
    "    target_image= Input(shape=(256, 256, 3))\n",
    "    \n",
    "    cat= Concatenate()([source_image, target_image]) ## this makes the Gan conditional\n",
    "    d= Conv2D(64, kernel_size=4, strides=2, padding=\"same\", \n",
    "                     kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "                     use_bias=False)(cat)\n",
    "    d=LeakyReLU(0.2)(d)\n",
    "    \n",
    "    \n",
    "    d=Conv2D(128, kernel_size=4, strides=2, padding=\"same\", \n",
    "                     kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "                     use_bias=False)(d)\n",
    "    d= BatchNormalization()(d)\n",
    "    d=LeakyReLU(0.2)(d)\n",
    "    \n",
    "    \n",
    "    d=Conv2D(256, kernel_size=4, strides=2, padding=\"same\", \n",
    "                     kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "                     use_bias=False)(d)\n",
    "    d= BatchNormalization()(d)\n",
    "    d=LeakyReLU(0.2)(d)\n",
    "    \n",
    "    \n",
    "    d=Conv2D(512, kernel_size=4, strides=2, padding=\"same\", \n",
    "                     kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "                     use_bias=False)(d)\n",
    "    d= BatchNormalization()(d)\n",
    "    d=LeakyReLU(0.2)(d)\n",
    "    \n",
    "    d=Conv2D(1, kernel_size=4, strides=1, padding=\"same\", \n",
    "                     kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "                     use_bias=False)(d)\n",
    "    \n",
    "    d= Activation(\"sigmoid\")(d)\n",
    "    return Model([source_image, target_image], d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1194bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 256, 256, 6)  0           ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 128, 128, 64  6144        ['concatenate[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 128, 128, 64  0           ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 64, 64, 128)  131072      ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64, 64, 128)  512        ['conv2d_1[0][0]']               \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 64, 64, 128)  0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 256)  524288      ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 32, 32, 256)  0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 16, 16, 512)  2097152     ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 16, 16, 512)  2048       ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 16, 16, 512)  0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 16, 16, 1)    8192        ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 16, 16, 1)    0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,770,432\n",
      "Trainable params: 2,768,640\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = build_discriminator()\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13a3eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Lets define the generator which is a clasic unet model but instead of double convs and maxpools \n",
    "###it uses single strided convs\n",
    "###encoder:C64-C128-C256-C512-C512-C512-C512-C512 ##the last one is bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35412d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(inputs, filters, batchnorm=True):\n",
    "    g= Conv2D(filters, kernel_size=4, strides=2, padding=\"same\", \n",
    "                     kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "                     use_bias=False)(inputs)\n",
    "    if batchnorm:\n",
    "        g= BatchNormalization()(g)\n",
    "                                \n",
    "    g= LeakyReLU(0.2)(g)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bf61e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###decoder: CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128 ## the first one is bottleneck the 1024 are actualy cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5a1e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(inputs, skips, filters, dropout=True):\n",
    "    g= Conv2DTranspose(filters, kernel_size=4, strides=2, padding=\"same\", \n",
    "                        kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False)(inputs)\n",
    "    g= BatchNormalization()(g)\n",
    "                                \n",
    "    \n",
    "    if dropout:\n",
    "        g=Dropout(0.5)(g)\n",
    "        \n",
    "    g= Concatenate()([g, skips])\n",
    "    g= Activation(\"relu\")(g)\n",
    "    return g    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e43d164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    input_image=Input(shape=(256, 256, 3))\n",
    "    \n",
    "    d1= encoder_block(input_image, 64, batchnorm=False) #128\n",
    "    d2= encoder_block(d1, 128) #64\n",
    "    d3= encoder_block(d2, 256) #32\n",
    "    d4= encoder_block(d3, 512) #16\n",
    "    d5= encoder_block(d4, 512) #8\n",
    "    d6= encoder_block(d5, 512) #4\n",
    "    d7= encoder_block(d6, 512) #2\n",
    "    \n",
    "    bottleneck= Conv2D(512, kernel_size=4, strides=2, padding=\"same\", \n",
    "                     kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), \n",
    "                     use_bias=False)(d7) #1\n",
    "    bottleneck=Activation(\"relu\")(bottleneck)\n",
    "    \n",
    "    u1= decoder_block(bottleneck, d7, 512) #2\n",
    "    u2 = decoder_block(u1, d6, 512) #4\n",
    "    u3= decoder_block(u2, d5, 512) #8\n",
    "    u4= decoder_block(u3, d4, 512, dropout=False)#16\n",
    "    u5= decoder_block(u4, d3, 256, dropout=False)#32\n",
    "    u6= decoder_block(u5, d2, 128, dropout=False)#64\n",
    "    u7= decoder_block(u6, d1, 64,dropout=False)#128\n",
    "    \n",
    "    final_conv= Conv2DTranspose(3, kernel_size=4, strides=2, padding=\"same\", \n",
    "                        kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False)(u7)###256\n",
    "    \n",
    "    out= Activation(\"tanh\")(final_conv)\n",
    "    return Model(input_image, out)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a3e263c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 128, 128, 64  3072        ['input_3[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 128, 128, 64  0           ['conv2d_5[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 64, 64, 128)  131072      ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 64, 64, 128)  0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 256)  524288      ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 32, 32, 256)  0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 16, 16, 512)  2097152     ['leaky_re_lu_6[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 16, 16, 512)  2048       ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 16, 16, 512)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 8, 8, 512)    4194304     ['leaky_re_lu_7[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 8, 8, 512)   2048        ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 8, 8, 512)    0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 4, 4, 512)    4194304     ['leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 4, 4, 512)   2048        ['conv2d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 4, 4, 512)    0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 2, 2, 512)    4194304     ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 2, 2, 512)   2048        ['conv2d_11[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_10 (LeakyReLU)     (None, 2, 2, 512)    0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 1, 1, 512)    4194304     ['leaky_re_lu_10[0][0]']         \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 1, 1, 512)    0           ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 2, 2, 512)   4194304     ['activation_1[0][0]']           \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 2, 2, 512)   2048        ['conv2d_transpose[0][0]']       \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 2, 2, 512)    0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 2, 2, 1024)   0           ['dropout[0][0]',                \n",
      "                                                                  'leaky_re_lu_10[0][0]']         \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 2, 2, 1024)   0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 4, 4, 512)   8388608     ['activation_2[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 4, 4, 512)   2048        ['conv2d_transpose_1[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4, 4, 512)    0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 4, 4, 1024)   0           ['dropout_1[0][0]',              \n",
      "                                                                  'leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 4, 4, 1024)   0           ['concatenate_2[0][0]']          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 8, 8, 512)   8388608     ['activation_3[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 8, 8, 512)   2048        ['conv2d_transpose_2[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 8, 8, 512)    0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 8, 8, 1024)   0           ['dropout_2[0][0]',              \n",
      "                                                                  'leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 8, 8, 1024)   0           ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 16, 16, 512)  8388608    ['activation_4[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_transpose_3[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 16, 16, 1024  0           ['batch_normalization_12[0][0]', \n",
      "                                )                                 'leaky_re_lu_7[0][0]']          \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 16, 16, 1024  0           ['concatenate_4[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_4 (Conv2DTran  (None, 32, 32, 256)  4194304    ['activation_5[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_transpose_4[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 32, 32, 512)  0           ['batch_normalization_13[0][0]', \n",
      "                                                                  'leaky_re_lu_6[0][0]']          \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 512)  0           ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_transpose_5 (Conv2DTran  (None, 64, 64, 128)  1048576    ['activation_6[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 64, 64, 128)  512        ['conv2d_transpose_5[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 64, 64, 256)  0           ['batch_normalization_14[0][0]', \n",
      "                                                                  'leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 64, 64, 256)  0           ['concatenate_6[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_transpose_6 (Conv2DTran  (None, 128, 128, 64  262144     ['activation_7[0][0]']           \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 128, 128, 64  256        ['conv2d_transpose_6[0][0]']     \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 128, 128, 12  0           ['batch_normalization_15[0][0]', \n",
      "                                8)                                'leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 128, 128, 12  0           ['concatenate_7[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_7 (Conv2DTran  (None, 256, 256, 3)  6144       ['activation_8[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 256, 256, 3)  0           ['conv2d_transpose_7[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 54,423,808\n",
      "Trainable params: 54,413,952\n",
      "Non-trainable params: 9,856\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = build_generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18ffe9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "binary_cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0598626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator loss\n",
    "def generator_loss(disc_generated_output, generated_images, target):\n",
    "    gan_loss = binary_cross_entropy(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - generated_images))\n",
    "    total_gen_loss = gan_loss + (100 * l1_loss)  # Lambda parameter balances the two losses\n",
    "\n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a53f5684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator loss\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = binary_cross_entropy(tf.ones_like(disc_real_output), disc_real_output)\n",
    "    fake_loss = binary_cross_entropy(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "    total_disc_loss = 0.5*(real_loss + fake_loss)\n",
    "\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36cfb12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate generator and discriminator\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a643b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "generator_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.5, beta_2=0.999)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.5, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2344bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_image, target):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # Generate fake image\n",
    "        generated_images = generator(input_image, training=True)\n",
    "\n",
    "        # Discriminator outputs\n",
    "        disc_real_output = discriminator([input_image, target], training=True)\n",
    "        disc_generated_output = discriminator([input_image, generated_images], training=True)\n",
    "\n",
    "        # Calculate losses\n",
    "        gen_total_loss = generator_loss(disc_generated_output, generated_images, target)\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "    # Calculate gradients\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    # Apply gradients\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
    "    return gen_total_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97733116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the training dataset: 1096\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "# Directory containing the combined images\n",
    "dataset_dir = \"dataset/train\"\n",
    "\n",
    "# List all image files in the directory\n",
    "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".jpg\")]\n",
    "\n",
    "\n",
    "\n",
    "# Create empty lists to store input and target images\n",
    "input_images = []\n",
    "target_images = []\n",
    "\n",
    "# Load and preprocess the images\n",
    "for file in image_files:\n",
    "    image_path = os.path.join(dataset_dir, file)\n",
    "    image = cv2.imread(image_path)\n",
    "    input_image = (tf.image.resize(image[:, :600, :], [256, 256])) \n",
    "    target_image = (tf.image.resize(image[:, 600:, :], [256, 256]))\n",
    "    input_image = (input_image - 127.5) / 127.5\n",
    "    target_image = (target_image - 127.5) / 127.5\n",
    "    input_images.append(input_image)\n",
    "    target_images.append(target_image)\n",
    "\n",
    "input_images = np.array(input_images)\n",
    "target_images = np.array(target_images)\n",
    "\n",
    "# Create TensorFlow dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((input_images, target_images))\n",
    "\n",
    "# Shuffle and batch the dataset\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1096).batch(1)\n",
    "\n",
    "# Print the number of samples in the training dataset\n",
    "print(\"Number of samples in the training dataset:\", len(train_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e71e9acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input images: (1096, 256, 256, 3)\n",
      "Shape of target images: (1096, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# Print shape information\n",
    "print(\"Shape of input images:\", input_images.shape)\n",
    "print(\"Shape of target images:\", target_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14390ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "881d1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=50\n",
    "display_interval=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b9f5ff9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/50:   0%|          | 0/1096 [00:00<?, ?batch/s]C:\\Users\\jpolo\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend.py:5673: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "Epoch 0/50: 100%|██████████| 1096/1096 [01:44<00:00, 10.46batch/s, Generator Loss=13.3, Discriminator Loss=0.136] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 1096/1096 [01:36<00:00, 11.38batch/s, Generator Loss=7.54, Discriminator Loss=0.378] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 1096/1096 [01:36<00:00, 11.33batch/s, Generator Loss=6.74, Discriminator Loss=0.183] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.25batch/s, Generator Loss=13.8, Discriminator Loss=0.0613] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.21batch/s, Generator Loss=11.9, Discriminator Loss=0.12]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 1096/1096 [01:36<00:00, 11.30batch/s, Generator Loss=12.6, Discriminator Loss=0.136] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 1096/1096 [01:35<00:00, 11.46batch/s, Generator Loss=6.33, Discriminator Loss=0.528]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 1096/1096 [01:36<00:00, 11.40batch/s, Generator Loss=8.39, Discriminator Loss=0.602]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 1096/1096 [01:36<00:00, 11.39batch/s, Generator Loss=10.6, Discriminator Loss=0.168]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 1096/1096 [01:35<00:00, 11.45batch/s, Generator Loss=12.1, Discriminator Loss=0.219]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 1096/1096 [01:35<00:00, 11.45batch/s, Generator Loss=14.8, Discriminator Loss=0.0141] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 1096/1096 [01:35<00:00, 11.44batch/s, Generator Loss=6.75, Discriminator Loss=1.52]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 1096/1096 [01:35<00:00, 11.44batch/s, Generator Loss=9.79, Discriminator Loss=0.136]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 1096/1096 [01:36<00:00, 11.38batch/s, Generator Loss=7.67, Discriminator Loss=0.169]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 1096/1096 [01:36<00:00, 11.32batch/s, Generator Loss=5.45, Discriminator Loss=0.316]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.23batch/s, Generator Loss=11.4, Discriminator Loss=0.423]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.27batch/s, Generator Loss=7.83, Discriminator Loss=0.116]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.23batch/s, Generator Loss=9.2, Discriminator Loss=0.19]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.24batch/s, Generator Loss=10.2, Discriminator Loss=0.432]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.26batch/s, Generator Loss=4.86, Discriminator Loss=0.339]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.22batch/s, Generator Loss=10.5, Discriminator Loss=0.231]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.22batch/s, Generator Loss=7.49, Discriminator Loss=0.17]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.21batch/s, Generator Loss=11.5, Discriminator Loss=0.752] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.19batch/s, Generator Loss=9.39, Discriminator Loss=0.0461] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.19batch/s, Generator Loss=13.1, Discriminator Loss=0.161]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 1096/1096 [01:38<00:00, 11.15batch/s, Generator Loss=10.1, Discriminator Loss=0.108]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.19batch/s, Generator Loss=12.5, Discriminator Loss=0.0177] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 1096/1096 [01:38<00:00, 11.15batch/s, Generator Loss=5.47, Discriminator Loss=1.09]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.19batch/s, Generator Loss=8.76, Discriminator Loss=0.0713] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 1096/1096 [01:38<00:00, 11.17batch/s, Generator Loss=12.5, Discriminator Loss=0.026]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 1096/1096 [01:38<00:00, 11.18batch/s, Generator Loss=7.09, Discriminator Loss=0.0125] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.19batch/s, Generator Loss=9.84, Discriminator Loss=0.187]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 1096/1096 [01:38<00:00, 11.18batch/s, Generator Loss=14.3, Discriminator Loss=0.111]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 1096/1096 [01:38<00:00, 11.18batch/s, Generator Loss=14.7, Discriminator Loss=0.0586] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.20batch/s, Generator Loss=9.38, Discriminator Loss=0.0301] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 1096/1096 [01:38<00:00, 11.18batch/s, Generator Loss=10.3, Discriminator Loss=0.21]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 1096/1096 [01:38<00:00, 11.18batch/s, Generator Loss=12.7, Discriminator Loss=0.0179] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.20batch/s, Generator Loss=7.99, Discriminator Loss=0.133]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.21batch/s, Generator Loss=12.1, Discriminator Loss=0.115]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.20batch/s, Generator Loss=9.86, Discriminator Loss=0.128]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.22batch/s, Generator Loss=10, Discriminator Loss=0.0596]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.21batch/s, Generator Loss=12, Discriminator Loss=0.00487]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.21batch/s, Generator Loss=7, Discriminator Loss=0.00998]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.22batch/s, Generator Loss=7.05, Discriminator Loss=0.0269] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.20batch/s, Generator Loss=6.23, Discriminator Loss=0.349] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 1096/1096 [01:36<00:00, 11.41batch/s, Generator Loss=10.3, Discriminator Loss=0.0663] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 1096/1096 [01:36<00:00, 11.40batch/s, Generator Loss=10.1, Discriminator Loss=0.0344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 1096/1096 [01:35<00:00, 11.43batch/s, Generator Loss=8.55, Discriminator Loss=0.151]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 1096/1096 [01:37<00:00, 11.20batch/s, Generator Loss=7.88, Discriminator Loss=0.026]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 1096/1096 [01:36<00:00, 11.37batch/s, Generator Loss=11, Discriminator Loss=0.0444]    \n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "\n",
    "    progress_bar = tqdm(train_dataset, desc=f'Epoch {epoch}/{num_epochs}', unit='batch')\n",
    "\n",
    "    for batch, (input_images, target_images) in enumerate(progress_bar):\n",
    "        gen_loss, disc_loss = train_step(input_images, target_images)\n",
    "\n",
    "        # Print losses for progress monitoring\n",
    "        progress_bar.set_postfix({'Generator Loss': gen_loss.numpy(), 'Discriminator Loss': disc_loss.numpy()})\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ab7e83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: generator\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: generator\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: discriminator\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: discriminator\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save the trained generator model\n",
    "from tensorflow.keras.models import save_model\n",
    "# Save the trained generator model\n",
    "generator.save(\"generator\")\n",
    "# Save the trained discriminator model\n",
    "discriminator.save(\"discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda76d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e7694c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c9d72c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
