import keras

import numpy as np
import tensorflow as tf
print(f"tensorflow version: {tf.__version__}")

from tensorflow.keras import layers, Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input,Conv2DTranspose,BatchNormalization,ReLU,Conv2D,LeakyReLU, Dropout, Activation
from tensorflow.keras.utils import image_dataset_from_directory
from tensorflow.keras.optimizers import Adam
from IPython import display
from tensorflow.keras.datasets import mnist

import time
import matplotlib.pyplot as plt

%matplotlib inline


import os
from os import listdir
from pathlib import Path
import imghdr

from tqdm.auto import tqdm


###Dataset https://www.kaggle.com/datasets/splcher/animefacedataset?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01
###Original DCGANS paper https://arxiv.org/abs/1511.06434


def plot_array(X,title=""):
    
    plt.rcParams['figure.figsize'] = (20,20) 

    for i,x in enumerate(X[0:5]):
        x=x.numpy()
        max_=x.max()
        min_=x.min()
        xnew=np.uint(255*(x-min_)/(max_-min_))
        plt.subplot(1,5,i+1)
        plt.imshow(xnew)
        plt.axis("off")

    plt.show()

img_height, img_width, batch_size=64,64,128
train_ds = tf.keras.utils.image_dataset_from_directory(directory='cartoon_20000', # change directory to 'cartoon_data' if you use the full dataset
                                                       image_size=(img_height, img_width),
                                                       batch_size=batch_size,
                                                       label_mode=None)

normalization_layer = layers.experimental.preprocessing.Rescaling(scale= 1./127.5, offset=-1)
normalized_ds = train_ds.map(lambda x: normalization_layer(x))

images=normalized_ds.take(1)
X=[x for x in images]
X
plot_array(X[0])

####Define the Generator

def build_generator():

    model = Sequential()
    
    
    model.add(Input(shape=(1, 1, 100)))
    model.add(Conv2DTranspose(64*16, kernel_size=4, strides=4, padding="valid", 
                        kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False))
    model.add(BatchNormalization(momentum=0.9,gamma_initializer=tf.keras.initializers.RandomNormal(mean=0., stddev=0.02),
                                         beta_initializer='zeros'
                                ))
    model.add(ReLU())
    
    model.add(Conv2DTranspose(64*8, kernel_size=4, strides=2, padding="same", 
                        kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False))
    model.add(BatchNormalization(momentum=0.9,gamma_initializer=tf.keras.initializers.RandomNormal(mean=0., stddev=0.02),
                                         beta_initializer='zeros'
                                ))
    model.add(ReLU())
    
    model.add(Conv2DTranspose(64*4, kernel_size=4, strides=2, padding="same", 
                        kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False))
    model.add(BatchNormalization(momentum=0.9,gamma_initializer=tf.keras.initializers.RandomNormal(mean=0., stddev=0.02),
                                         beta_initializer='zeros'
                                ))
    model.add(ReLU())
    
    model.add(Conv2DTranspose(64*2, kernel_size=4, strides=2, padding="same", 
                        kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False)),
    model.add(ReLU())
    
    model.add(Conv2DTranspose(3, kernel_size=4, strides=2, padding="same", 
                        kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False, 
                              #activation="tanh"
                             ))
    model.add(Activation("tanh"))

    
    

    return model


generator = build_generator()
generator.summary()


###Define the discriminator

def build_discriminator():


    model = Sequential()
    model.add(Input(shape=(64,64,3)))
    
    model.add(Conv2D(64, kernel_size=4, strides=2, padding="same", 
                     kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), 
                     use_bias=False))
    model.add(LeakyReLU(0.2))
    
    model.add(Conv2D(64*2, kernel_size=4, strides=2, padding="same", 
                     kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), 
                     use_bias=False))
    model.add(BatchNormalization(momentum=0.9, 
                                 gamma_initializer=tf.keras.initializers.RandomNormal(mean=0., stddev=0.02),
                                 beta_initializer='zeros'
                                ))
    model.add(LeakyReLU(0.2))
    
    model.add(Conv2D(64*4, kernel_size=4, strides=2, padding="same", 
                     kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), 
                     use_bias=False))
    model.add(BatchNormalization(momentum=0.9, 
                                 gamma_initializer=tf.keras.initializers.RandomNormal(mean=0., stddev=0.02),
                                 beta_initializer='zeros'
                                ))
    model.add(LeakyReLU(0.2))
    
    
    
    model.add(Conv2D(64*8, kernel_size=4, strides=2, padding="same", 
                     kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), 
                     use_bias=False))
    model.add(BatchNormalization(momentum=0.9, 
                                 gamma_initializer=tf.keras.initializers.RandomNormal(mean=0., stddev=0.02),
                                 beta_initializer='zeros'
                                ))
    model.add(LeakyReLU(0.2))
    
    model.add(Conv2D(1, kernel_size=4, strides=2, padding="valid", 
                     kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), 
                     use_bias=False, #activation="sigmoid"
                    ))
    model.add(Activation("sigmoid"))
    
    

    

    return model


discriminator = build_discriminator()
discriminator.summary()

import os

def save_generated_images(generator, epoch):
    r, c = 5, 5  # Number of rows and columns of generated images
    noise = np.random.normal(0, 1, (r * c,) + noise_shape)
    generated_images = generator.predict(noise)

    # Rescale generated images to [0, 1]
    generated_images = 0.5 * generated_images + 0.5

    # Create a directory to save the generated images
    save_dir = "generated_images_mnist"
    os.makedirs(save_dir, exist_ok=True)

    # Save the generated images
    for i in range(r * c):
        img = generated_images[i, :, :, :]
        img = np.squeeze(img)  # Remove the channel dimension if exists
        img = (img * 255).astype(np.uint8)  # Scale to [0, 255]
        file_path = os.path.join(save_dir, f"generated_image_epoch_{epoch}_sample_{i}.png")
        plt.imsave(file_path, img, cmap='gray')

    print(f"Generated images saved for epoch {epoch}.")

cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)

def generator_loss(Xhat):
    return cross_entropy(tf.ones_like(Xhat), Xhat)

def discriminator_loss(X, Xhat):
    real_loss = cross_entropy(tf.ones_like(X), X)
    fake_loss = cross_entropy(tf.zeros_like(Xhat), Xhat)
    total_loss = 0.5*(real_loss + fake_loss)
    return total_loss

learning_rate = 0.0002

generator_optimizer = tf.keras.optimizers.Adam(lr = 0.0002, beta_1 = 0.5, beta_2 = 0.999 )

discriminator_optimizer = tf.keras.optimizers.Adam(lr = 0.0002, beta_1 = 0.5, beta_2 = 0.999 )


@tf.function

def train_step(X):
    
    z= tf.random.normal([BATCH_SIZE, 1, 1, latent_dim])
    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        #generated sample 
        xhat = generator(z, training=True)
        #the output of the discriminator for real data 
        real_output = discriminator(X, training=True)
        #the output of the discriminator for fake data
        fake_output = discriminator(xhat, training=True)
        
        #loss for each 
        gen_loss= generator_loss(fake_output)
        disc_loss = discriminator_loss(real_output, fake_output)
      # Compute the gradients for gen_loss and generator
    
    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    # Compute the gradients for gen_loss and discriminator
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)
    # Ask the optimizer to apply the processed gradients
    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

epochs=150

discriminator=build_discriminator()

generator= build_generator()


start_t = time.time()
for epoch in range(epochs):
    print("Epoch", epoch + 1)

    num_batches = len(normalized_ds)
    
    with tqdm(total=num_batches, unit='batch') as pbar:
        for images in normalized_ds:
            train_step(images)
            pbar.update(1)
            pbar.set_postfix({'Remaining Time': pbar.format_interval(pbar.n / (time.time() - start_t))})

    z = tf.random.normal([BATCH_SIZE, 1, 1, latent_dim])
    generated_images = generator(z, training=False)
    
    X=[x for x in normalized_ds]
    print("orignal images")
    plot_array(X[0])
    print("generated images")
    plot_array(generated_images)
    

